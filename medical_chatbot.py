import streamlit as st
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --------------------------------------------------
# ENV
# --------------------------------------------------
load_dotenv()

DB_FAISS_PATH = "vectorstore/db_FAISS"

def is_greeting(text: str) -> bool:
    greetings = ["hi", "hello", "hey", "good morning", "good evening"]
    return text.lower().strip() in greetings

# --------------------------------------------------
# CACHE: VECTORSTORE + RAG CHAIN
# --------------------------------------------------
@st.cache_resource
def load_rag_chain():
    # LLM
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.5
    )

    # Prompt
    prompt = ChatPromptTemplate.from_template("""
    You are a medical information assistant.

    You must answer the user's question using ONLY the information provided in the context.
    Do NOT use any outside knowledge.
    Do NOT make assumptions.
    Do NOT generalize.

    If the context does NOT contain enough information to answer the question, reply EXACTLY with:
    "I don't know based on the provided context."

    Context:
    {context}

    Question:
    {question}

    Answer:

    Note: This information is for educational purposes only and is not a substitute for professional medical advice.
    """)

    # Embeddings + FAISS
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    db = FAISS.load_local(
        DB_FAISS_PATH,
        embedding_model,
        allow_dangerous_deserialization=True
    )

    retriever = db.as_retriever(search_kwargs={"k": 3})

    # LCEL RAG chain
    rag_chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain, retriever


# --------------------------------------------------
# STREAMLIT APP
# --------------------------------------------------
def main():
    st.title("ü©∫ Medical RAG Chatbot")
    st.caption(
        "‚ö†Ô∏è This chatbot provides information for educational purposes only and "
        "is not a substitute for professional medical advice."
    )
    


    rag_chain, retriever = load_rag_chain()

    # Init chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Replay chat history
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # Chat input
    user_query = st.chat_input("Ask a medical question")

    if user_query:
        # User message
        st.chat_message("user").markdown(user_query)
        st.session_state.messages.append(
            {"role": "user", "content": user_query}
        )

        # üëâ GREETING HANDLING FIRST
        if is_greeting(user_query):
            response = (
                "Hi üëã I‚Äôm your medical information assistant."
            )

        else:
            # üîç ONLY retrieve context for real medical questions
            with st.expander("üîç Retrieved context"):
                docs = retriever.invoke(user_query)
                for i, d in enumerate(docs, 1):
                    st.markdown(f"**Chunk {i}:**")
                    st.write(d.page_content)

            with st.spinner("Thinking..."):
                response = rag_chain.invoke(user_query)

        # Assistant response
        st.chat_message("assistant").markdown(response)
        st.session_state.messages.append(
            {"role": "assistant", "content": response}
        )

if __name__ == "__main__":
    main()
